{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgfhFoA8WI9J"
      },
      "source": [
        "#GPT NEO based discord chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Eq1gbpUWR1h"
      },
      "source": [
        "## Discord API\n",
        "You just need to insert your discord API and then run all the cells"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hSR9YYeVaPJ"
      },
      "source": [
        "discord_token = '' #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2NFHj4tWY9z"
      },
      "source": [
        "##Module installation\n",
        "this will install all the necessary modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsJmPcofa006"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install discord.py\n",
        "!pip install nest_asyncio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ai3jougHXOfS"
      },
      "source": [
        "##Download and load GPT NEO model.\n",
        "It will take a little bit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyudeREf1uZQ"
      },
      "source": [
        "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
        "\n",
        "model = GPTNeoForCausalLM.from_pretrained('EleutherAI/gpt-neo-1.3B')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('EleutherAI/gpt-neo-1.3B')\n",
        "\n",
        "# add padding token to tokenizer\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "# set padding token id to the id of the padding token\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrBHO29EzInM"
      },
      "source": [
        "##Mode setup\n",
        "You can use this cell to select or create the desired mode\n",
        "To create a mode just follow this example:\n",
        "\n",
        "\n",
        "```python\n",
        "modes['mode_name'] = { 'prompt' : 'insert text that is put at the beginning of the input',\n",
        "\n",
        "message'\n",
        "                      }\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pjn1cPP75RAk"
      },
      "source": [
        "modes = dict()\n",
        "\n",
        "modes['chat'] = {'prompt': 'Self Generated AI Humanoid',\n",
        "            }\n",
        "\n",
        "mode = modes['chat']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FjWp2WizM2J"
      },
      "source": [
        "##Function to get GPT's answer from discord chat history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMY3qq5c5tmN"
      },
      "source": [
        "You can play with time_limit and max_length using the sliders. Time limit is very useful if you want fast replies by GPT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hspd_O3maydb"
      },
      "source": [
        "time_limit = 4 #@param {type:\"slider\", min:1, max:100, step:1}\n",
        "max_length = 813 #@param {type:\"slider\", min:100, max:5000, step:1}\n",
        "\n",
        "def AI_answer(string):\n",
        "  in_string = mode['prompt'] + string\n",
        "  inputs = tokenizer.encode(in_string, return_tensors='pt', truncation=True, max_length=512)\n",
        "  inputs = inputs.cuda()\n",
        "  attention_mask = inputs.ne(tokenizer.pad_token_id).float()\n",
        "  outputs = model.generate(inputs, max_length=max_length, do_sample=True, max_time=time_limit, attention_mask=attention_mask)\n",
        "  text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "  stripped_text = text[len(in_string):]\n",
        "\n",
        "  #preprocessing answer\n",
        "  return stripped_text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9nnXEh6zUXd"
      },
      "source": [
        "##Discord bot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKWSksHfd7NO"
      },
      "source": [
        "import discord\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "client = discord.Client(intents=discord.Intents.default())\n",
        "\n",
        "@client.event\n",
        "async def on_ready():\n",
        "    print('We have logged in as {0.user}'.format(client))\n",
        "\n",
        "@client.event\n",
        "async def on_message(message):\n",
        "    if message.author == client.user:\n",
        "        return\n",
        "\n",
        "    messages = []\n",
        "    async for m in message.channel.history():\n",
        "        messages.append(m)\n",
        "\n",
        "    #create input string\n",
        "    in_string = ''\n",
        "    for message in reversed(messages):\n",
        "      \n",
        "      if message.author == client.user:\n",
        "\n",
        "        output = AI_answer(in_string)\n",
        "\n",
        "    await message.channel.send(output)\n",
        "\n",
        "client.run(discord_token)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}